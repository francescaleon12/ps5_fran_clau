---
title: "Problem Set 5"
author: "Francesca Leon and Claudia Felipe"
date: "11/05/2024"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Francesca leon (francescaleon)
    - Partner 2 (name and cnet ID): Claudia Felipe (claudiafelipe)
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: **FL** **CF**
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: **0** Late coins left after submission: **4**
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
from bs4 import BeautifulSoup
import requests
from tabulate import tabulate
from datetime import datetime

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
# Get the HTML
soup = BeautifulSoup(requests.get("https://oig.hhs.gov/fraud/enforcement/").\
  content, "html.parser")

# Find and store titles
titles = soup.find_all("h2", class_="usa-card__heading")
title_list = []
for title in titles:
    title = title.find("a").text.strip()
    title_list.append(title)

# Find and store links
links = soup.find_all("h2", class_="usa-card__heading")
link_list = []
for link in links:
    link = link.find("a").get("href")
    link_list.append("https://oig.hhs.gov" + link)

# Find and store dates
dates = soup.find_all("span", class_="text-base-dark")
date_list = []
for date in dates:
    date = date.text.strip()
    date_list.append(date)

# Find and store categories
categories = soup.find_all("ul", class_="display-inline")
category_list = []
for category in categories:
    category = category.find("li").text.strip()
    category_list.append(category)

# Merge values
values = pd.DataFrame({
    "Title": title_list,
    "Link": link_list,
    "Date": date_list,
    "Category": category_list
})

# Print head
values.head()
```

  
### 2. Crawling (PARTNER 1)

```{python}
# List to store agencies
agencies = []

# Loop through each link
for link in values["Link"]:
        response = requests.get(link)
        soup_agency = BeautifulSoup(response.content, "html.parser")
        agency = soup_agency.find("span", text="Agency:")
        agencies.append(agency.next_sibling.strip())

# Add column to the dataframe
values["Agency"] = agencies

# Print head
values.head()
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

1. Check if the year is 2013 or later. If not, ask the user to enter a valid year.

2. Initialize empty lists for storing titles, links, dates, and categories.

3. Start web scraping as in Step 1, using a loop to go through each page.

4. For each page:
    - Extract title, link, date, and category.
    - Append this information to the respective lists.
    - Identify the earliest date on the current page.

5. If the earliest date on the current page is earlier than the user-specified start date, break the loop.

6. After the loop, create a DataFrame from the lists.

7. Filter the DataFrame to keep only records with dates from the user-specified date onward.

8. Save the final DataFrame to a CSV file.


* b. Create Dynamic Scraper (PARTNER 2)

```{python}
def dynamic_scraper(month, year):
    # Check if the year is 2013 or later
    if year < 2013:
        print("Please enter a year of 2013 or later.")
        return None
    
    start_date = datetime(year, month, 1)
    
    # Initialize empty lists to store data
    title_list, link_list, date_list, category_list = [], [], [], []
    
    # Begin looping through pages for web scraping
    page = 1
    stop_scraping = False  
    
    while not stop_scraping:
        url = f"https://oig.hhs.gov/fraud/enforcement/?page={page}"
        response = requests.get(url)
        soup = BeautifulSoup(response.content, "html.parser")
        
        titles = soup.find_all("h2", class_="usa-card__heading")
        dates = soup.find_all("span", class_="text-base-dark")
        categories = soup.find_all("ul", class_="display-inline")
        
        for i in range(len(titles)):
            # Extract title and link
            title_text = titles[i].find("a").text.strip()
            link = titles[i].find("a").get("href")
            full_link = "https://oig.hhs.gov" + link
            
            # Extract date 
            date_text = dates[i].text.strip()
            date_object = datetime.strptime(date_text, "%B %d, %Y")
            
            # If the date is older than the specified start date, break
            if date_object < start_date:
                stop_scraping = True
                break
            
            # Extract category
            category_text = categories[i].find("li").text.strip()
            
            # Append data to lists
            title_list.append(title_text)
            link_list.append(full_link)
            date_list.append(date_object)
            category_list.append(category_text)
        
        time.sleep(1)
        page += 1

    # Create a DataFrame from the collected lists
    data = pd.DataFrame({
        "Title": title_list,
        "Link": link_list,
        "Date": date_list,
        "Category": category_list
    })

    # Save the filtered DataFrame to a CSV file
    filename = f"enforcement_actions_{year}_{month}.csv"
    data.to_csv(filename, index=False)
    print(f"Data saved to {filename}")

    return data

# For January 2023
enforcement_actions= dynamic_scraper(1, 2023)  
print(enforcement_actions.head())

# Count the number of enforcement actions in the DataFrame
num_actions = enforcement_actions.shape[0]
print(f"Number of enforcement actions scraped: {num_actions}")

# Earliest enforcement action
enforcement_actions_sorted = enforcement_actions.sort_values(by="Date", ascending=True)
earliest_action = enforcement_actions_sorted.iloc[0]
print("\nEarliest enforcement action scraped:")
print(earliest_action)
```

* c. Test Partner's Code (PARTNER 1)

```{python}
# From January 2021
enforcement_actions_2021 = dynamic_scraper(1, 2021)  

# Count the number of enforcement actions in the DataFrame
num_actions_2021 = enforcement_actions_2021.shape[0]
print(f"Number of enforcement actions scraped: {num_actions_2021}")

# Earliest enforcement action
enforcement_actions_2021_sorted = enforcement_actions_2021.sort_values(by="Date", ascending=True)
earliest_action_2021 = enforcement_actions_2021_sorted.iloc[0]
print("\nEarliest enforcement action scraped since 2021:")
print(earliest_action_2021)
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```